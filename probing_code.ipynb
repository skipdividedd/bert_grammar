{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nseed = 159\nos.environ['PYTHONHASHSEED']=str(seed)\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\" \nos.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" ","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:04.185490Z","iopub.execute_input":"2023-03-21T09:18:04.186494Z","iopub.status.idle":"2023-03-21T09:18:04.218769Z","shell.execute_reply.started":"2023-03-21T09:18:04.186456Z","shell.execute_reply":"2023-03-21T09:18:04.217837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/fdalvi/NeuroX","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:04.220828Z","iopub.execute_input":"2023-03-21T09:18:04.221297Z","iopub.status.idle":"2023-03-21T09:18:05.952228Z","shell.execute_reply.started":"2023-03-21T09:18:04.221256Z","shell.execute_reply":"2023-03-21T09:18:05.950726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\npackage_paths = [\n    '/kaggle/working/NeuroX/',\n]\n\nfor pth in package_paths:\n    sys.path.append(pth)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:05.954087Z","iopub.execute_input":"2023-03-21T09:18:05.956181Z","iopub.status.idle":"2023-03-21T09:18:05.962820Z","shell.execute_reply.started":"2023-03-21T09:18:05.956107Z","shell.execute_reply":"2023-03-21T09:18:05.961612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom collections import OrderedDict\nfrom IPython.display import clear_output","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:05.966231Z","iopub.execute_input":"2023-03-21T09:18:05.967032Z","iopub.status.idle":"2023-03-21T09:18:05.975753Z","shell.execute_reply.started":"2023-03-21T09:18:05.966993Z","shell.execute_reply":"2023-03-21T09:18:05.974434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.__version__","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:05.979687Z","iopub.execute_input":"2023-03-21T09:18:05.980687Z","iopub.status.idle":"2023-03-21T09:18:09.591030Z","shell.execute_reply.started":"2023-03-21T09:18:05.980643Z","shell.execute_reply":"2023-03-21T09:18:09.589099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:09.592429Z","iopub.execute_input":"2023-03-21T09:18:09.594882Z","iopub.status.idle":"2023-03-21T09:18:09.611076Z","shell.execute_reply.started":"2023-03-21T09:18:09.594831Z","shell.execute_reply":"2023-03-21T09:18:09.608278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    \n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n    torch.use_deterministic_algorithms(True)\n    \n    random.seed(seed)\n    torch.manual_seed(seed)\n\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \n    \nset_seed(seed)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:09.612778Z","iopub.execute_input":"2023-03-21T09:18:09.613179Z","iopub.status.idle":"2023-03-21T09:18:09.635779Z","shell.execute_reply.started":"2023-03-21T09:18:09.613124Z","shell.execute_reply":"2023-03-21T09:18:09.633580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.set_num_threads(1)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:09.637740Z","iopub.execute_input":"2023-03-21T09:18:09.639246Z","iopub.status.idle":"2023-03-21T09:18:09.647935Z","shell.execute_reply.started":"2023-03-21T09:18:09.639204Z","shell.execute_reply":"2023-03-21T09:18:09.646685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_path = '/kaggle/input/taiga-pos'\npath_work = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:09.649916Z","iopub.execute_input":"2023-03-21T09:18:09.651178Z","iopub.status.idle":"2023-03-21T09:18:09.659972Z","shell.execute_reply.started":"2023-03-21T09:18:09.651125Z","shell.execute_reply":"2023-03-21T09:18:09.658866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from NeuroX.neurox.data.extraction import transformers_extractor","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:09.665475Z","iopub.execute_input":"2023-03-21T09:18:09.666228Z","iopub.status.idle":"2023-03-21T09:18:19.145573Z","shell.execute_reply.started":"2023-03-21T09:18:09.666186Z","shell.execute_reply":"2023-03-21T09:18:19.144429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from NeuroX.neurox.data.writer import ActivationsWriter","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.147167Z","iopub.execute_input":"2023-03-21T09:18:19.148002Z","iopub.status.idle":"2023-03-21T09:18:19.154146Z","shell.execute_reply.started":"2023-03-21T09:18:19.147957Z","shell.execute_reply":"2023-03-21T09:18:19.153035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Конвертируем файл, который получили из Probing_framework (уже получили, не в этом ноутбуке)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.155866Z","iopub.execute_input":"2023-03-21T09:18:19.156728Z","iopub.status.idle":"2023-03-21T09:18:19.165683Z","shell.execute_reply.started":"2023-03-21T09:18:19.156688Z","shell.execute_reply":"2023-03-21T09:18:19.164558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_and_tokenizer(model_path, device=\"cpu\", random_weights=False):\n\n    model = AutoModel.from_pretrained(model_path, output_hidden_states=True).to(device)\n    tokenizer = AutoTokenizer.from_pretrained('cointegrated/rubert-tiny2')\n\n    if random_weights:\n        print(\"Randomizing weights\")\n        model.init_weights()\n\n    return model, tokenizer","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.169210Z","iopub.execute_input":"2023-03-21T09:18:19.169481Z","iopub.status.idle":"2023-03-21T09:18:19.178523Z","shell.execute_reply.started":"2023-03-21T09:18:19.169455Z","shell.execute_reply":"2023-03-21T09:18:19.177378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_representations(\n    model_desc,\n    input_corpus,\n    output_file,\n    device=\"cpu\",\n    aggregation=\"last\",\n    output_type=\"json\",\n    random_weights=False,\n    ignore_embeddings=False,\n    decompose_layers=False,\n    filter_layers=None,\n):\n    print(f\"Loading model: {model_desc}\")\n    model, tokenizer = get_model_and_tokenizer(\n        model_desc, device=device, random_weights=random_weights\n    )\n\n    print(\"Reading input corpus\")\n\n    def corpus_generator(input_corpus_path):\n        with open(input_corpus_path, \"r\") as fp:\n            for line in fp:\n                yield line.strip()\n            return\n\n    print(\"Preparing output file\")\n    writer = ActivationsWriter.get_writer(output_file, filetype=output_type, decompose_layers=decompose_layers, filter_layers=filter_layers)\n\n    print(\"Extracting representations from model\")\n    tokenization_counts = {} # Cache for tokenizer rules\n    for sentence_idx, sentence in enumerate(corpus_generator(input_corpus)):\n        hidden_states, extracted_words = transformers_extractor.extract_sentence_representations(\n            sentence,\n            model,\n            tokenizer,\n            device=device,\n            include_embeddings=(not ignore_embeddings),\n            aggregation=aggregation,\n            tokenization_counts=tokenization_counts\n        )\n\n        print(\"Hidden states: \", hidden_states.shape)\n        print(\"# Extracted words: \", len(extracted_words))\n\n        writer.write_activations(sentence_idx, extracted_words, hidden_states)\n\n    writer.close()","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.180302Z","iopub.execute_input":"2023-03-21T09:18:19.181057Z","iopub.status.idle":"2023-03-21T09:18:19.192236Z","shell.execute_reply.started":"2023-03-21T09:18:19.181018Z","shell.execute_reply":"2023-03-21T09:18:19.190987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_seed = 12345","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.193920Z","iopub.execute_input":"2023-03-21T09:18:19.195031Z","iopub.status.idle":"2023-03-21T09:18:19.209213Z","shell.execute_reply.started":"2023-03-21T09:18:19.195002Z","shell.execute_reply":"2023-03-21T09:18:19.208100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ConvertSample:\n    \"\"\"\"\n    Gets .csv files, makes train & test split in .txt format, trying to balance data.\n    \"\"\"\n    \n    def __init__(self, path, train_size=2500, test_size=900, shuffle: bool = True): \n\n        self.shuffle = shuffle\n        self.path = path\n        self.project_path = str(Path(os.getcwd()).parents[0])\n        self.category = re.search(r'[a-zA-Z]+_[a-zA-Z]+(?=.csv)', path)[0]\n        self.train_size = train_size\n        self.test_size = test_size\n        \n\n    def read(self) -> list: \n        with open(self.path, encoding=\"utf-8\") as f:\n            lines = [line.split('\\t') for line in f]\n            \n            if self.shuffle:\n                random.seed(data_seed)\n                random.shuffle(lines)\n                \n        return lines\n    \n    def stupid_cycle(self, values, dct, number) -> dict: #util для семплинга\n        \n        dict_filter = OrderedDict()\n        \n        for value in values:\n            i = 0\n            for k, v in dct.items():\n                if v == value:\n                    if i < number:\n                        dict_filter[k] = v\n                        i+=1\n            \n        return dict_filter\n    \n    \n    def stupid_test(self, values, dct) -> dict: #util для семплинга\n        \n        dict_filter = OrderedDict()\n        \n        for value in values:\n            i = 0\n            for k, v in dct.items():\n                if v == value:\n                    dict_filter[k] = v\n                    \n        return dict_filter\n                \n    def stupid_sampler(self) -> dict: #семплинг данных\n        \n        sents = self.read()\n        values_train = []\n        values_test = []\n        sents_train = []\n        sents_test = []\n\n        for line in sents:\n            part, value, sentence = line[0], line[1], line[2]\n            if 2 < len(sentence.split()) < 35:\n                if part == 'tr':\n                    if sentence not in sents_train:\n                        values_train.append(value)\n                        sents_train.append(sentence)\n                    \n                if part == 'te' or part== 'va':\n                    if sentence not in sents_train and sentence not in sents_test:\n                        values_test.append(value)\n                        sents_test.append(sentence)\n        \n        \n        train_dict = OrderedDict(zip(sents_train, values_train))\n        test_dict = OrderedDict(zip(sents_test, values_test))\n\n        A = set(values_train)\n        B = set(values_test)\n        values = sorted(list(A.intersection(B)))\n        \n        length = len(values)\n            \n        number_one = round(self.train_size/length)\n\n        dict_filter_train = self.stupid_cycle(values, train_dict, number_one)\n        dict_filter_test = self.stupid_test(values, test_dict)\n            \n        return dict_filter_train, dict_filter_test\n\n    def permute(self, dct) -> dict: # перемешивает словарь данных\n        \n        l = list(dct.items())\n        random.seed(data_seed)\n        random.shuffle(l)\n        return OrderedDict(l)\n        \n    def using_shuffle(self, a):\n        \n        keys = list(a.keys())\n        values = list(a.values())\n        random.seed(data_seed)\n        random.shuffle(values)\n        d = OrderedDict(zip(keys, values))\n        return d\n\n    def create_dicts(self):\n        \n        dict_filter_train, dict_filter_test = self.stupid_sampler()\n\n        if self.shuffle:\n            dict_filter_train = self.permute(dict_filter_train)\n            dict_filter_test = self.permute(dict_filter_test)\n        \n        d1 = dict(list(dict_filter_train.items())[len(dict_filter_train)//3:])\n        d2 = dict(list(dict_filter_train.items())[:len(dict_filter_train)//3])\n        dict_filter_train = d1.copy()\n        dict_filter_test.update(d2)\n        dict_control_task = dict_filter_train.copy()\n        dict_control_task = self.using_shuffle(dict_control_task)\n\n        return dict_filter_train, dict_filter_test, dict_control_task\n\n\n    def create_paths(self) -> str:\n        \n        if re.search(r'(?<=\\/)[a-zA-Z][a-zA-Z]_[a-zA-Z]+(?=_)', self.path)[0]:\n            dataset = re.search(r'(?<=\\/)[a-zA-Z][a-zA-Z]_[a-zA-Z]+(?=_)', self.path)[0]\n            path = path_work+f'/large_data_{dataset}'\n        else:\n            path = path_work+'/large_data'\n            \n        if not os.path.isdir(path):\n            os.mkdir(path)\n            \n        if not os.path.isdir(path+f'/data_{self.category}'):\n            os.mkdir(path+f'/data_{self.category}')\n        \n        result_path_datatrain = path+f\"/data_{self.category}/datatrain_{self.category}.txt\"\n        result_path_labeltrain = path+f\"/data_{self.category}/labeltrain_{self.category}.txt\"\n        \n        result_path_cdatatrain = path+f\"/data_{self.category}/cdatatrain_{self.category}.txt\"\n        result_path_clabeltrain = path+f\"/data_{self.category}/clabeltrain_{self.category}.txt\"\n        \n        result_path_datatest = path+f\"/data_{self.category}/datatest_{self.category}.txt\"\n        result_path_labeltest = path+f\"/data_{self.category}/labeltest_{self.category}.txt\"\n\n        return result_path_datatrain, result_path_labeltrain, result_path_cdatatrain, result_path_clabeltrain, \\\n               result_path_datatest, result_path_labeltest\n\n\n    def writer(self) -> str: \n        \"\"\"\n        Writes to a file\n        \"\"\"\n        result_datatrain, result_labeltrain, result_cdatatrain, result_clabeltrain, result_datatest, result_labeltest = self.create_paths()\n       \n        \n        dict_filter_train, dict_filter_test, dict_control_task = self.create_dicts()\n\n        with open(result_datatrain, \"w\", encoding=\"utf-8\") as traindata, \\\n             open(result_labeltrain, \"w\", encoding=\"utf-8\") as trainlabel, \\\n             open(result_cdatatrain, \"w\", encoding=\"utf-8\") as ctraindata, \\\n             open(result_clabeltrain, \"w\", encoding=\"utf-8\") as ctrainlabel, \\\n             open(result_datatest, \"w\", encoding=\"utf-8\") as testdata, \\\n             open(result_labeltest, \"w\", encoding=\"utf-8\") as testlabel:\n            \n    \n            for sentence, value in dict_filter_train.items():\n                traindata.writelines(sentence)\n                trainlabel.writelines(value + '\\n')\n\n            for sentence, value in dict_control_task .items():\n                ctraindata.writelines(sentence)\n                ctrainlabel.writelines(value + '\\n')\n\n\n            for sentence, value in dict_filter_test.items():\n                testdata.writelines(sentence)\n                testlabel.writelines(value + '\\n')\n                                                                  \n        \n        return result_datatrain, result_labeltrain, result_cdatatrain, result_clabeltrain, result_datatest, result_labeltest\n        \n\n\nclass GetEmbeddings:\n    \"\"\"\"\n    Receives .txt files with sentences and computes embeddings for them.\n    \"\"\"\n    \n    def __init__(self, path_trdata, path_tedata):\n        \n        self.path_trdata = path_trdata\n        self.path_tedata = path_tedata\n        \n        self.category = re.search(r'[a-zA-Z]+_[a-zA-Z]+(?=.txt)', path_trdata)[0]\n        self.dataset = re.search(r'(?<=_)[a-zA-Z]+_[a-zA-Z]+(?=\\/)', path_trdata)[0]\n        \n    def jsons(self, model):\n        \n        path = path_work + f'/large_data_{self.dataset}/data_{self.category}'\n        \n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        print('Using device:', device)\n        print()\n        \n        extract_representations(model,\n        self.path_trdata,\n        path+'/activations_train.json',\n        aggregation=\"average\", #last, first   \n        device=device                                            \n        )\n        \n        clear_output(wait=False)\n        print('Using device:', device)\n        print()\n        \n        extract_representations(model,\n        self.path_tedata,\n        path+'/activations_te.json',\n        aggregation=\"average\", #last, first\n        device=device                                               \n        )\n        clear_output(wait=False)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.210869Z","iopub.execute_input":"2023-03-21T09:18:19.211452Z","iopub.status.idle":"2023-03-21T09:18:19.244129Z","shell.execute_reply.started":"2023-03-21T09:18:19.211408Z","shell.execute_reply":"2023-03-21T09:18:19.243092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://neurox.qcri.org/docs/neurox.data.extraction.html?highlight=extract_representations#neurox.data.extraction.transformers_extractor.extract_representations","metadata":{}},{"cell_type":"code","source":"import pickle\nfrom NeuroX.neurox.data import loader as data_loader\nfrom NeuroX.neurox.interpretation import utils\nfrom NeuroX.neurox.interpretation import ablation\nfrom NeuroX.neurox.interpretation import linear_probe","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:19.247632Z","iopub.execute_input":"2023-03-21T09:18:19.247898Z","iopub.status.idle":"2023-03-21T09:18:20.164900Z","shell.execute_reply.started":"2023-03-21T09:18:19.247873Z","shell.execute_reply":"2023-03-21T09:18:20.163683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_sentence_data(source_path, labels_path, activations): \n    \n    #тут немного переписали функцию потому что в библиотеке ошибка!!!\n\n    tokens = {\"source\": [], \"target\": []}\n\n    with open(source_path) as source_fp:\n        for line_idx, line in enumerate(source_fp):\n            line_tokens = line.strip().split() #вот тут переписано\n            tokens[\"source\"].append(line_tokens) #и тут\n\n    with open(labels_path) as labels_fp:\n        for line in labels_fp:\n            line_tokens = line.strip().split()\n            tokens[\"target\"].append(line_tokens)\n\n    assert len(tokens[\"source\"]) == len(tokens[\"target\"]), (\n        \"Number of lines do not match (source: %d, target: %d)!\"\n        % (len(tokens[\"source\"]), len(tokens[\"target\"]))\n    )\n\n    assert len(activations) == len(tokens[\"source\"]), (\n        \"Number of lines do not match (activations: %d, source: %d)!\"\n        % (len(activations), len(tokens[\"source\"]))\n    )\n\n    \n    for idx, activation in enumerate(activations):\n        assert activation.shape[0] == len(tokens[\"source\"][idx])\n\n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:20.169803Z","iopub.execute_input":"2023-03-21T09:18:20.172872Z","iopub.status.idle":"2023-03-21T09:18:20.186546Z","shell.execute_reply.started":"2023-03-21T09:18:20.172831Z","shell.execute_reply":"2023-03-21T09:18:20.184793Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nfrom torch.autograd import Variable\n\ndef _numpyfy(x):\n    if isinstance(x, np.ndarray):\n        return x\n    return np.array(x)\n\n\ndef accuracy(preds, labels):\n    preds = _numpyfy(preds)\n    labels = _numpyfy(labels)\n    return (preds == labels).mean()\n\n\nclass LinearProbe(nn.Module):\n    \"\"\"Torch model for linear probe\"\"\"\n    \n    def __init__(self, input_size, num_classes):\n        \"\"\"Initialize a linear model\"\"\"\n        super(LinearProbe, self).__init__()\n        self.linear = nn.Linear(input_size, num_classes)\n\n    def forward(self, x):\n        \"\"\"Run a forward pass on the model\"\"\"\n        out = self.linear(x)\n        return out\n\ndef l1_penalty(var):\n    return torch.abs(var).sum()\n\n\ndef l2_penalty(var):\n    return torch.sqrt(torch.pow(var, 2).sum())\n\n\ndef _train_probe(\n    X_train,\n    y_train,\n    task_type,\n    lambda_l1=0,\n    lambda_l2=0,\n    num_epochs=10,\n    batch_size=32,\n    learning_rate=0.001,\n    ):\n\n    progressbar = utils.get_progress_bar()\n    print(\"Training %s probe\" % (task_type))\n    # Check if we can use GPU's for training\n    use_gpu = torch.cuda.is_available()\n\n    if lambda_l1 is None or lambda_l2 is None:\n        raise ValueError(\"Regularization weights cannot be None\")\n\n    print(\"Creating model...\")\n    if task_type == \"classification\":\n        num_classes = len(set(y_train))\n        if num_classes <= 1:\n            raise ValueError(\n                \"Classification problem must have more than one target class\"\n            )\n    else:\n        num_classes = 1\n    print(\"Number of training instances:\", X_train.shape[0])\n    if task_type == \"classification\":\n        print(\"Number of classes:\", num_classes)\n    \n    set_seed(seed)\n    probe = LinearProbe(X_train.shape[1], num_classes)\n    \n    if use_gpu:\n        probe = probe.cuda()\n\n    if task_type == \"classification\":\n        criterion = nn.CrossEntropyLoss()\n    elif task_type == \"regression\":\n        criterion = nn.MSELoss()\n    else:\n        raise ValueError(\"Invalid `task_type`\")\n    \n    set_seed(seed)\n    optimizer = torch.optim.Adam(probe.parameters(), lr=learning_rate)\n\n    X_tensor = torch.from_numpy(X_train)\n    y_tensor = torch.from_numpy(y_train)\n\n    for epoch in range(num_epochs):\n        num_tokens = 0\n        avg_loss = 0\n        for inputs, labels in progressbar(\n            utils.batch_generator(X_tensor, y_tensor, batch_size=batch_size),\n            desc=\"epoch [%d/%d]\" % (epoch + 1, num_epochs),\n        ):\n            num_tokens += inputs.shape[0]\n            if use_gpu:\n                inputs = inputs.cuda()\n                labels = labels.cuda()\n            inputs = inputs.float()\n            inputs = Variable(inputs)\n            labels = Variable(labels)\n            \n            print('INPUTS')\n            print(inputs)\n            print('one input')\n            print(inputs[0])\n            set_seed(seed)\n            # Forward + Backward + Optimize\n            optimizer.zero_grad()\n            \n            outputs = probe(inputs)\n            \n\n            \n            if task_type == \"regression\":\n                outputs = outputs.squeeze()\n            weights = list(probe.parameters())[0]\n            \n            set_seed(seed)\n            loss = (\n                criterion(outputs, labels)\n                + lambda_l1 * l1_penalty(weights)\n                + lambda_l2 * l2_penalty(weights)\n            )\n            \n            set_seed(seed)\n            loss.backward()\n            \n            set_seed(seed)\n            optimizer.step()\n            \n           \n            avg_loss += loss.item()\n            \n        print(\n            \"Epoch: [%d/%d], Loss: %.4f\"\n            % (epoch + 1, num_epochs, avg_loss / num_tokens)\n        )\n\n    return probe\n\n\ndef train_logistic_regression_probe(\n    X_train,\n    y_train,\n    lambda_l1=0,\n    lambda_l2=0,\n    num_epochs=10,\n    batch_size=32,\n    learning_rate=0.001,\n    ):\n\n    return _train_probe(\n        X_train,\n        y_train,\n        task_type=\"classification\",\n        lambda_l1=lambda_l1,\n        lambda_l2=lambda_l2,\n        num_epochs=num_epochs,\n        batch_size=batch_size,\n        learning_rate=learning_rate,\n    )\n\ndef compute_score(preds, labels, metric):\n\n    if metric == \"accuracy\":\n        return accuracy(preds, labels)\n\n\ndef evaluate_probe(\n    probe,\n    X,\n    y,\n    idx_to_class=None,\n    return_predictions=False,\n    source_tokens=None,\n    batch_size=32,\n    metric=\"accuracy\",\n    ):\n \n    progressbar = utils.get_progress_bar()\n\n    # Check if we can use GPU's for evaluation\n    use_gpu = torch.cuda.is_available()\n\n    if use_gpu:\n        probe = probe.cuda()\n\n    # always evaluate in full precision\n    probe = probe.float()\n\n    # Test the Model\n    y_pred = []\n\n    def source_generator():\n        for s in source_tokens:\n            for t in s:\n                yield t\n\n    src_words = source_generator()\n\n    if return_predictions:\n        predictions = []\n        src_word = -1\n\n    for inputs, labels in progressbar(\n        utils.batch_generator(\n            torch.from_numpy(X), torch.from_numpy(y), batch_size=batch_size\n        ),\n        desc=\"Evaluating\",\n        ):\n        if use_gpu:\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n\n        # always evaluate in full precision\n        inputs = inputs.float()\n\n        inputs = Variable(inputs)\n        labels = Variable(labels)\n\n        outputs = probe(inputs)\n\n        if outputs.data.shape[1] == 1:\n            # Regression\n            predicted = outputs.data\n        else:\n            # Classification\n            _, predicted = torch.max(outputs.data, 1)\n        predicted = predicted.cpu().numpy()\n\n        for i in range(0, len(predicted)):\n            idx = predicted[i]\n            if idx_to_class:\n                key = idx_to_class[idx]\n            else:\n                key = idx\n\n            y_pred.append(predicted[i])\n\n            if return_predictions:\n                if source_tokens:\n                    src_word = next(src_words)\n                else:\n                    src_word = src_word + 1\n                predictions.append((src_word, key, labels[i].item() == idx))\n\n    y_pred = np.array(y_pred)\n\n    result = compute_score(y_pred, y, metric)\n\n    print(\"Score (%s) of the probe: %0.2f\" % (metric, result))\n\n    class_scores = {}\n    class_scores[\"__OVERALL__\"] = result\n\n    if idx_to_class:\n        for i in idx_to_class:\n            class_name = idx_to_class[i]\n            class_instances_idx = np.where(y == i)[0]\n            y_pred_filtered = y_pred[class_instances_idx]\n            y_filtered = y[class_instances_idx]\n            total = y_filtered.shape\n            if total == 0:\n                class_scores[class_name] = 0\n            else:\n                class_scores[class_name] = compute_score(\n                    y_pred_filtered, y_filtered, metric\n                )\n\n    if return_predictions:\n        return class_scores, predictions\n    return class_scores","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:20.191406Z","iopub.execute_input":"2023-03-21T09:18:20.194704Z","iopub.status.idle":"2023-03-21T09:18:20.243302Z","shell.execute_reply.started":"2023-03-21T09:18:20.194655Z","shell.execute_reply":"2023-03-21T09:18:20.242224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Experiment:\n    \n    \n    def __init__(self, path_trdata, path_trlabel, path_tedata, path_telabel):\n\n        #некрасиво инициализирована куча переменных для функций\n        \n        self.path_trdata = path_trdata\n        self.path_trlabel = path_trlabel\n        self.path_tedata = path_tedata\n        self.path_telabel = path_telabel\n        self.category = re.search(r'[a-zA-Z]+_[a-zA-Z]+(?=.txt)', path_trdata)[0]\n        self.dataset = re.search(r'(?<=_)[a-zA-Z]+_[a-zA-Z]+(?=\\/)', path_trdata)[0]\n        \n        self.path = path_work+f'large_data_{self.dataset}/data_{self.category}'\n        \n        self.activations_tr, self.num_layers = data_loader.load_activations(self.path+'/activations_train.json', 768)\n        self.activations_te, self.num_layers = data_loader.load_activations(self.path+'/activations_te.json', 768)\n        \n        self.tokens_tr = load_sentence_data(self.path_trdata, self.path_trlabel, self.activations_tr)\n        self.tokens_te = load_sentence_data(self.path_tedata, self.path_telabel, self.activations_te)\n        \n#         self.X_tr, self.y_tr, mp = utils.create_tensors(self.tokens_tr, self.activations_tr, 'Nom')\n#         self.label2idx, self.idx2label, self.src2idx, self.idx2src = mp\n\n#         self.X_te, self.y_te, mapping = utils.create_tensors(self.tokens_te, self.activations_te, 'Nom', mappings = mp)\n\n        self.X_tr, self.y_tr, self.mapping = utils.create_tensors(self.tokens_tr, self.activations_tr, 'Nom')\n        label2id, id2label, self.src2idx, self.idx2src = self.mapping\n        labels = sorted(label2id.keys())\n        indeces = [int(i) for i in range(len(labels))]\n        k = np.array(self.y_tr, dtype=np.uint32)\n        self.y_tr = [id2label[y] for y in k]\n        self.label2idx = dict(zip(labels, indeces))\n        self.idx2label = {v: k for k, v in self.label2idx.items()}\n        self.y_tr = [self.label2idx[y] for y in self.y_tr]\n        self.y_tr = np.array(self.y_tr, dtype=np.int)\n        mpp = self.label2idx, self.idx2label, self.src2idx, self.idx2src\n        self.X_te, self.y_te, mapping = utils.create_tensors(self.tokens_te, self.activations_te, 'Nom', mappings = mpp)\n\n        \n    def run_classification(self):#just пробинг\n           \n        probe = train_logistic_regression_probe(self.X_tr, self.y_tr, lambda_l1=0.003, lambda_l2=0.003, batch_size=64)\n        scores_tr = evaluate_probe(probe, self.X_tr, self.y_tr, idx_to_class=self.idx2label, batch_size=64)\n        scores_te = evaluate_probe(probe, self.X_te, self.y_te, idx_to_class=self.idx2label, batch_size=64)\n        return probe, scores_tr, scores_te\n    \n    def nranking(self, probe): #тут топ нейроны\n\n        ordering, cutoffs = linear_probe.get_neuron_ordering(probe, self.label2idx, search_stride=100)\n        return ordering, cutoffs\n    \n    def top_n(self, probe, percentage=0.2):\n\n        return linear_probe.get_top_neurons(probe, percentage, self.label2idx) #return np.array(list(top_neurons_union)), top_neurons (dict)\n    \n    def threshold_n(self, probe, fraction=2):\n        return linear_probe.get_top_neurons_hard_threshold(probe, fraction, self.label2idx) #np.array(list(top_neurons_union)), top_neurons\n    \n    def keep_bottom(self, neurons):\n        X_tr_b = deepcopy(self.X_tr)\n        X_te_b = deepcopy(self.X_te)\n        X_tr_selected = ablation.filter_activations_remove_neurons(X_tr_b, neurons)\n        probe_selected = linear_probe.train_logistic_regression_probe(X_tr_selected, self.y_tr, lambda_l1=0.003, lambda_l2=0.003)\n        scores_tr = linear_probe.evaluate_probe(probe_selected, X_tr_selected, self.y_tr, idx_to_class=self.idx2label)\n        X_te_selected = ablation.filter_activations_remove_neurons(X_te_b, neurons)\n        scores_te = linear_probe.evaluate_probe(probe_selected, X_te_selected, self.y_te, idx_to_class=self.idx2label)\n        return scores_tr, scores_te\n    \n    def keep_util(self, neurons, X_tr, X_te):\n        X_tr_selected = ablation.filter_activations_keep_neurons(X_tr, neurons)\n        probe_selected = linear_probe.train_logistic_regression_probe(X_tr_selected, self.y_tr, lambda_l1=0.003, lambda_l2=0.003)\n        scores_tr = linear_probe.evaluate_probe(probe_selected, X_tr_selected, self.y_tr, idx_to_class=self.idx2label)\n        X_te_selected = ablation.filter_activations_keep_neurons(X_te, neurons)\n        scores_te = linear_probe.evaluate_probe(probe_selected, X_te_selected, self.y_te, idx_to_class=self.idx2label)\n        return scores_tr, scores_te\n    \n    def return_weights(self, probe):\n        weights1 = list(probe.parameters())[0].data.cpu()\n        weights2 = np.abs(weights1.numpy())\n        return weights1, weights2\n        \n    def keep_only(self, neurons, goal='top'):\n       \n        if goal == 'top':\n            X_tr_top = deepcopy(self.X_tr)\n            X_te_top = deepcopy(self.X_te)\n            return self.keep_util(neurons, X_tr_top, X_te_top)\n            \n        elif goal == 'threshold':\n            X_tr_t = deepcopy(self.X_tr)\n            X_te_t = deepcopy(self.X_te)\n            return self.keep_util(neurons, X_tr_t, X_te_t)  \n        \n        \n    def data_size(self):\n        return self.X_tr.shape[0], self.X_te.shape[0], len(set(self.y_te))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:20.248309Z","iopub.execute_input":"2023-03-21T09:18:20.251170Z","iopub.status.idle":"2023-03-21T09:18:20.285874Z","shell.execute_reply.started":"2023-03-21T09:18:20.251098Z","shell.execute_reply":"2023-03-21T09:18:20.284683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for dirname, _, filenames in os.walk(data_path):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:20.290169Z","iopub.execute_input":"2023-03-21T09:18:20.293572Z","iopub.status.idle":"2023-03-21T09:18:20.307389Z","shell.execute_reply.started":"2023-03-21T09:18:20.293532Z","shell.execute_reply":"2023-03-21T09:18:20.306212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordered_neurons = {}\nordered_neurons_c = {}\nthreshold = {}\nthreshold_c = {}\nweights = {}\nweights_c = {}\n\ntop_n = {}\ntop_n_per_class = {}\ntop_n_c = {}\ntop_n_per_class_c = {}\n\nbottom_n = {}\n# bottom_n2 = {}\n\nscores = {}\nscores_control = {}\n\nsize = {}\n\nscores_keep_top = {}\nscores_keep_top_c = {}\n\nscores_keep_thres = {}\nscores_keep_thres_c = {}\n\nscores_keep_bot = {}\n# scores_keep_bot2 = {}\n\n\n\n\nfor dirname, _, filenames in os.walk(data_path):\n    for filename in filenames:\n        file = os.path.join(dirname, filename)\n\n        splitter = ConvertSample(file)\n\n        #получаем трейновую и тестовую выборку\n        path_trdata, path_trlabel,path_ctrdata, path_ctrlabel, path_tedata, path_telabel = splitter.writer()\n        #получаем эмбеддинги\n        data = GetEmbeddings(path_trdata, path_tedata)\n        data.jsons('/kaggle/input/broken-1kk')\n        cat = Experiment(path_trdata, path_trlabel, path_tedata, path_telabel)    \n        cat_name = re.search(r'[a-zA-Z]+_[a-zA-Z]+(?=.txt)', path_trdata)[0]\n        d_name = cat.dataset\n        X_tr_shape, X_te_shape, n_class = cat.data_size()\n        size[cat_name] = [X_tr_shape, X_te_shape, n_class]\n        \n        probe, scores_tr, scores_te = cat.run_classification() # просто классификация\n        scores[cat_name] = [scores_tr, scores_te]\n        weights1, weights2 = cat.return_weights(probe)\n        weights[cat_name] = [weights1, weights2]\n        \n        ordering = cat.top_n(probe, percentage=1)[1] # ранжирование\n        ordered_neurons[cat_name] = ordering\n        \n        top_n[cat_name] = cat.top_n(probe)[0]\n        top_n_per_class[cat_name] = cat.top_n(probe)[1]\n        scores_tr, scores_te = cat.keep_only(neurons=top_n[cat_name], goal='top') \n        scores_keep_top[cat_name] = [scores_tr, scores_te] # на топ процентов\n        \n        bottom_n[cat_name] = cat.top_n(probe, percentage=0.8)[0]\n        scores_tr, scores_te = cat.keep_bottom(neurons=bottom_n[cat_name])\n        scores_keep_bot[cat_name] = [scores_tr, scores_te]   # на bottom процентов\n        \n#         bottom_n2[cat_name] = cat.top_n(probe, percentage=0.98)[0]\n#         scores_tr, scores_te = cat.keep_bottom(neurons=bottom_n2[cat_name])\n#         scores_keep_bot2[cat_name] = [scores_tr, scores_te]   # на bottom процентов\n        \n        \n        threshold[cat_name] = cat.threshold_n(probe)[0] \n        scores_tr, scores_te = cat.keep_only(threshold[cat_name], goal='threshold') \n        scores_keep_thres[cat_name] = [scores_tr, scores_te] # с трешхолдом\n        \n        \n        with open(f'scores_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores, f)\n            \n        with open(f'label2dx_{d_name}.pkl', 'wb') as f:\n            pickle.dump(cat.label2idx, f)\n            \n        with open(f'neurons_{d_name}.pkl', 'wb') as f:\n            pickle.dump(ordered_neurons, f)\n        \n        with open(f'weights_{d_name}.pkl', 'wb') as f:\n            pickle.dump(weights, f)\n            \n        with open(f'scores_keep_top_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores_keep_top, f)\n            \n        with open(f'scores_keep_thres_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores_keep_thres, f)\n            \n        with open(f'scores_keep_bot_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores_keep_bot, f)\n            \n#         with open(f'scores_keep_bot2_{d_name}.pkl', 'wb') as f:\n#             pickle.dump(scores_keep_bot2, f)\n            \n        with open(f'top_n_{d_name}.pkl', 'wb') as f:\n            pickle.dump(top_n, f)\n            \n        with open(f'top_n_per_class_{d_name}.pkl', 'wb') as f:\n            pickle.dump(top_n_per_class, f)\n            \n        with open(f'bottom_n_{d_name}.pkl', 'wb') as f:\n            pickle.dump(bottom_n, f)\n            \n#         with open(f'bottom_n2_{d_name}.pkl', 'wb') as f:\n#             pickle.dump(bottom_n2, f)\n            \n        with open(f'threshold_{d_name}.pkl', 'wb') as f:\n            pickle.dump(threshold, f)\n            \n        with open(f'size_{d_name}.pkl', 'wb') as f:\n            pickle.dump(size, f)\n            \n        cat = Experiment(path_ctrdata, path_ctrlabel, path_tedata, path_telabel)    \n        cat_name = re.search(r'[a-zA-Z]+_[a-zA-Z]+(?=.txt)', path_trdata)[0]\n        \n        probe, scores_tr, scores_te = cat.run_classification()\n        scores_control[cat_name] = [scores_tr, scores_te]\n        \n        weights1_c, weights2_c = cat.return_weights(probe)\n        weights_c[cat_name] = [weights1_c, weights2_c]\n        \n        ordering_c = cat.top_n(probe, percentage=1)[1] # ранжирование\n        ordered_neurons_c[cat_name] = ordering_c\n        \n        top_n_c[cat_name] = cat.top_n(probe)[0] \n        top_n_per_class_c[cat_name] = cat.top_n(probe)[1]\n        scores_tr, scores_te = cat.keep_only(neurons=top_n_c[cat_name], goal='top')\n        scores_keep_top_c[cat_name] = [scores_tr, scores_te] # на топ процентов\n        \n        threshold_c[cat_name] = cat.threshold_n(probe)[0] \n        scores_tr, scores_te = cat.keep_only(threshold_c[cat_name], goal='threshold') \n        scores_keep_thres_c[cat_name] = [scores_tr, scores_te] # с трешхолдом\n        \n        with open(f'top_n_per_class_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(top_n_per_class_c, f)\n        \n        with open(f'top_n_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(top_n_c, f)\n        \n        with open(f'label2dx_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(cat.label2idx, f)\n            \n        with open(f'weights_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(weights_c, f)\n        \n        with open(f'scores_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores_control, f)\n        \n        with open(f'scores_keep_top_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores_keep_top_c, f)\n            \n        with open(f'scores_keep_thres_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(scores_keep_thres_c, f)\n            \n        with open(f'neurons_c_{d_name}.pkl', 'wb') as f:\n            pickle.dump(ordered_neurons_c, f)\n         \n\n\n        dir_to_delete = f'/kaggle/working/large_data_{d_name}/data_{cat_name}/' \n        with os.scandir(dir_to_delete) as entries:\n            for entry in entries:\n                file_to_delete = f\"{dir_to_delete}{entry.name}\"\n                print(file_to_delete)\n                os.remove(file_to_delete)","metadata":{"execution":{"iopub.status.busy":"2023-03-21T09:18:20.312221Z","iopub.execute_input":"2023-03-21T09:18:20.312910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}